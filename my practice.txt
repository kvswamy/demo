

MYSQL
alter table table_name add column columnname TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;

alter table tablename drop column columnname;
update tablename set columnname=100 whre name ='veera';

alter table pokemon add column pk int primary key auto_increment;

delete from pokemon where id >= 802  ;

alter table pokemon drop column id;

alter table pokemon change pk id int;

alter table air add primary key(sno);

alter table m rename to p;

alter table flight modify eff_dt_till timestamp;

alter table flight MODIFY COLUMN eff_dt_till timestamp NULL;

delete from flight;

SQOOP
echo -n "root" >sqoop.password

/////content in options file
--connect 
jdbc:mysql://localhost/db 
--username 
root

sqoop list-tables --options-file /home/gopal/opt.opt --password-file file:///home/gopal/sqoop.password   ///in options file connect credentials




for managed tables:
sqoop import --connect jdbc:mysql://localhost/db --username root -P --table pokemon --target-dir /user/M --append --incremental lastmodified --check-column last_modified --last-value '2018-09-02 13:03:08.0' -m 1 

sqoop import --connect jdbc:mysql://localhost/db --username root -P --table pokemon --target-dir /user/M  --incremental append --check-column id --last-value 807 -m 1 

sqoop codegen --connect jdbc:mysql://localhost/db --username root -P --table pokemon 

sqoop import --connect jdbc:mysql://localhost/db --username root -P --table pokemon --hive-import --create-hive-table --hive-table pokemon1 --warehouse-dir /user/hive/warehouse/ -m 1     /////////this will create hive table automatically with provided name need not to create another in hive//////////

sqoop import --connect jdbc:mysql://localhost/db --username root -P --table pokemon --target-dir /user/P -m 1 --append

sqoop import-all-tables --connect jdbc:mysql://localhost/db --username root -P --warehouse-dir /user/P -m 1

sqoop list-tables --connect jdbc:mysql://localhost/db --username root -P

sqoop list-databases --connect jdbc:mysql://localhost/ --username root -P

sqoop eval --connect jdbc:mysql://localhost/db --username root -P --query "alter table m drop column id "

for external tables
sqoop job --create sqoop_loan_info1 -- import --connect jdbc:mysql://localhost/db --username root -P --table loan_info --target-dir /bank/loan_info -m 1 

sqoop job --list
sqoop job --exec sqoop_loan_info1
then 
create external table in hive asfollows
create external table loan_info(id int,lp_date date)row format delimited fields terminated by',' location'/bank/loaninfo';

note:location must be target dir in sqoop job so that data will point no need to load data into hive table.



HIVE

set hive.exec.dynamic.partition = true;

set hive.exec.dynamic.partition.mode = nonstrict;

 create table par(m1 int,m2 int,id1 int)partitioned by(year string);/////////////////upto this partitioning////////////////////////////////////////

insert overwrite table par partition(year) select * from m;

create table tab as select * from m;

select max(length(sno)) from air;

create table if not exists temp(sno double,LOCAL_AIRPORT string,AIRLINE_CODE string,FLIGHT_NUMBER int,FREQUENCY double,ARRIVAL_DEPARTURE_FLAG int,SOURCE_DESTINATION string,SCHED_TIME double,TIME_ZONE string,EFF_DT_FROM string,EFF_DT_TILL string) row format delimited fields terminated by',' tblproperties("skip.header.line.count"="1")

create table if not exists flight(sno double,LOCAL_AIRPORT string,AIRLINE_CODE string,FLIGHT_NUMBER int,FREQUENCY double,ARRIVAL_DEPARTURE_FLAG int,SOURCE_DESTINATION string,SCHED_TIME double,TIME_ZONE string,EFF_DT_FROM timestamp,EFF_DT_TILL timestamp) row format delimited fields terminated by',' tblproperties("skip.header.line.count"="1")


insert overwrite table flight select sno,LOCAL_AIRPORT,AIRLINE_CODE,FLIGHT_NUMBER,FREQUENCY,ARRIVAL_DEPARTURE_FLAG,SOURCE_DESTINATION,SCHED_TIME,TIME_ZONE,from_unixtime(unix_timestamp(EFF_DT_FROM,"YYYY-MM-DD'T'HH:MM:SS")),from_unixtime(unix_timestamp(EFF_DT_TILL,"YYYY-MM-DD'T'HH:MM:SS")) from temp;



create table if not exists air(sno double,airport string,as_code int ,as_desc string ,icon_sel_url string,icon_unsel_url string,asc_name string,asc_code int,asc_desc string,img_url string, email string,contact string,url string) row format delimited fields terminated by',' stored as textfile tblproperties("skip.header.line.count"="1") ;


create table if not exists census(sno int,age int,emp_type string,income double,emp string,total int,marital_status string,dept string,family_type string,race string,gender string,income1 int,income2 int,cat_id int,nationality string,population string) row format delimited fields terminated by',' lines terminated by'\n' stored as textfile tblproperties("skip.header.line.count"="1");

try this link for partitioning and bucketing:
https://acadgild.com/blog/bucketing-in-hive


bucketing and partitioning:
create table bucketing(street string,city string,zip int,state string,beds int,baths int,sq_ft double, type string,price double) row format delimited fields terminated by',' lines terminated by'\n' stored as textfile tblproperties("skip.header.line.count"="1"); 

load data local inpath'/home/gopal/real_csv'into table bucketing;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions=20000;
set hive.exec.max.dynamic.partitions.pernode=20000;
set hive.enforce.bucketing=true; 

create table bucket_tb(street string,zip int,state string,beds int,baths int,sq_ft double, type string,price double)partitioned by (city string) clustered by(street) into 4 buckets  row format delimited fields terminated by',' lines terminated by'\n' stored as textfile ; 

insert into table bucket_tb partition(city) select street,zip,state,beds,baths,sq_ft,type,price,city from bucketing;
